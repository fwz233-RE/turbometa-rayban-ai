/*
 * Qwen-Omni-Realtime WebSocket Service
 * Provides real-time audio and video chat with AI
 */

import Foundation
import UIKit
import AVFoundation

// MARK: - WebSocket Events

enum OmniClientEvent: String {
    case sessionUpdate = "session.update"
    case inputAudioBufferAppend = "input_audio_buffer.append"
    case inputAudioBufferCommit = "input_audio_buffer.commit"
    case inputImageBufferAppend = "input_image_buffer.append"
    case responseCreate = "response.create"
}

enum OmniServerEvent: String {
    case sessionCreated = "session.created"
    case sessionUpdated = "session.updated"
    case inputAudioBufferSpeechStarted = "input_audio_buffer.speech_started"
    case inputAudioBufferSpeechStopped = "input_audio_buffer.speech_stopped"
    case inputAudioBufferCommitted = "input_audio_buffer.committed"
    case responseCreated = "response.created"
    case responseAudioTranscriptDelta = "response.audio_transcript.delta"
    case responseAudioTranscriptDone = "response.audio_transcript.done"
    case responseAudioDelta = "response.audio.delta"
    case responseAudioDone = "response.audio.done"
    case responseDone = "response.done"
    case conversationItemCreated = "conversation.item.created"
    case conversationItemInputAudioTranscriptionCompleted = "conversation.item.input_audio_transcription.completed"
    case error = "error"
}

// MARK: - Service Class

class OmniRealtimeService: NSObject {

    // WebSocket
    private var webSocket: URLSessionWebSocketTask?
    private var urlSession: URLSession?

    // Configuration
    private let apiKey: String
    private let model = "qwen3-omni-flash-realtime"
    private let baseURL = "wss://dashscope.aliyuncs.com/api-ws/v1/realtime"

    // Audio Engine (for recording)
    private var audioEngine: AVAudioEngine?

    // Audio Playback Engine (separate engine for playback)
    private var playbackEngine: AVAudioEngine?
    private var playerNode: AVAudioPlayerNode?
    private let audioFormat = AVAudioFormat(commonFormat: .pcmFormatInt16, sampleRate: 24000, channels: 1, interleaved: true)

    // Audio buffer management
    private var audioBuffer = Data()
    private var isCollectingAudio = false
    private var audioChunkCount = 0
    private let minChunksBeforePlay = 2 // é¦–æ¬¡æ”¶åˆ°2ä¸ªç‰‡æ®µåå¼€å§‹æ’­æ”¾
    private var hasStartedPlaying = false
    private var isPlaybackEngineRunning = false

    // Callbacks
    var onTranscriptDelta: ((String) -> Void)?
    var onTranscriptDone: ((String) -> Void)?
    var onUserTranscript: ((String) -> Void)? // ç”¨æˆ·è¯­éŸ³è¯†åˆ«ç»“æœ
    var onAudioDelta: ((Data) -> Void)?
    var onAudioDone: (() -> Void)?
    var onSpeechStarted: (() -> Void)?
    var onSpeechStopped: (() -> Void)?
    var onError: ((String) -> Void)?
    var onConnected: (() -> Void)?
    var onFirstAudioSent: (() -> Void)?
    var onDisconnected: ((String) -> Void)?  // æ–­å¼€è¿æ¥å›è°ƒï¼Œå‚æ•°æ˜¯åŸå› 

    // State
    private var isRecording = false
    private var hasAudioBeenSent = false
    private var eventIdCounter = 0
    private var isDisconnecting = false  // æ ‡è¯†æ˜¯å¦æ­£åœ¨æ–­å¼€è¿æ¥
    
    // éŸ³é¢‘é‡é‡‡æ ·
    private var audioConverter: AVAudioConverter?
    private let targetFormat = AVAudioFormat(commonFormat: .pcmFormatFloat32, sampleRate: 24000, channels: 1, interleaved: false)

    init(apiKey: String) {
        self.apiKey = apiKey
        super.init()
        setupAudioEngine()
    }

    // MARK: - Audio Engine Setup

    private func setupAudioEngine() {
        // Recording engine
        audioEngine = AVAudioEngine()

        // Playback engine (separate from recording)
        setupPlaybackEngine()
    }

    private func setupPlaybackEngine() {
        playbackEngine = AVAudioEngine()
        playerNode = AVAudioPlayerNode()

        guard let playbackEngine = playbackEngine,
              let playerNode = playerNode,
              let audioFormat = audioFormat else {
            print("âŒ [Omni] æ— æ³•åˆå§‹åŒ–æ’­æ”¾å¼•æ“")
            return
        }

        // Attach player node
        playbackEngine.attach(playerNode)

        // Connect player node to output
        playbackEngine.connect(playerNode, to: playbackEngine.mainMixerNode, format: audioFormat)

        print("âœ… [Omni] æ’­æ”¾å¼•æ“åˆå§‹åŒ–å®Œæˆ: PCM16 @ 24kHz")
    }

    private func startPlaybackEngine() {
        guard let playbackEngine = playbackEngine, !isPlaybackEngineRunning else { return }

        do {
            try playbackEngine.start()
            isPlaybackEngineRunning = true
            print("â–¶ï¸ [Omni] æ’­æ”¾å¼•æ“å·²å¯åŠ¨")
        } catch {
            print("âŒ [Omni] æ’­æ”¾å¼•æ“å¯åŠ¨å¤±è´¥: \(error)")
        }
    }

    private func stopPlaybackEngine() {
        guard let playbackEngine = playbackEngine, isPlaybackEngineRunning else { return }

        // é‡è¦ï¼šå…ˆé‡ç½® playerNode ä»¥æ¸…é™¤æ‰€æœ‰å·²è°ƒåº¦ä½†æœªæ’­æ”¾çš„ buffer
        playerNode?.stop()
        playerNode?.reset()  // æ¸…é™¤é˜Ÿåˆ—ä¸­çš„æ‰€æœ‰ buffer
        playbackEngine.stop()
        isPlaybackEngineRunning = false
        print("â¹ï¸ [Omni] æ’­æ”¾å¼•æ“å·²åœæ­¢å¹¶æ¸…é™¤é˜Ÿåˆ—")
    }

    // MARK: - WebSocket Connection

    func connect() {
        // é‡ç½®æ–­å¼€æ ‡å¿—
        isDisconnecting = false
        hasAudioBeenSent = false
        
        let urlString = "\(baseURL)?model=\(model)"
        print("ğŸ”Œ [Omni] å‡†å¤‡è¿æ¥ WebSocket: \(urlString)")

        guard let url = URL(string: urlString) else {
            print("âŒ [Omni] æ— æ•ˆçš„ URL")
            onError?("Invalid URL")
            return
        }

        var request = URLRequest(url: url)
        request.setValue("Bearer \(apiKey)", forHTTPHeaderField: "Authorization")

        let configuration = URLSessionConfiguration.default
        urlSession = URLSession(configuration: configuration, delegate: self, delegateQueue: OperationQueue())

        webSocket = urlSession?.webSocketTask(with: request)
        webSocket?.resume()

        print("ğŸ”Œ [Omni] WebSocket ä»»åŠ¡å·²å¯åŠ¨")
        receiveMessage()

        // Wait a bit then send session configuration
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
            print("âš™ï¸ [Omni] å‡†å¤‡é…ç½®ä¼šè¯")
            self.configureSession()
        }
    }

    func disconnect() {
        guard !isDisconnecting else {
            print("ğŸ”Œ [Omni] å·²åœ¨æ–­å¼€ä¸­ï¼Œè·³è¿‡é‡å¤è°ƒç”¨")
            return
        }
        isDisconnecting = true
        print("ğŸ”Œ [Omni] æ–­å¼€ WebSocket è¿æ¥")
        stopRecording()
        stopPlaybackEngine()
        webSocket?.cancel(with: .goingAway, reason: nil)
        webSocket = nil
    }

    // MARK: - Session Configuration

    private func configureSession() {
        let sessionConfig: [String: Any] = [
            "event_id": generateEventId(),
            "type": OmniClientEvent.sessionUpdate.rawValue,
            "session": [
                "modalities": ["text", "audio"],
                "voice": "Cherry",
                "input_audio_format": "pcm16",
                "output_audio_format": "pcm24",
                "smooth_output": true,
                "instructions": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ã€‚\n\nã€é‡è¦ã€‘å¿…é¡»å§‹ç»ˆç”¨ä¸­æ–‡å›ç­”ã€‚\n\nå›ç­”è¦ç®€ç»ƒã€å£è¯­åŒ–ï¼Œåƒæœ‹å‹èŠå¤©ä¸€æ ·ã€‚ä¸è¦å•°å—¦ï¼Œç›´æ¥è¯´é‡ç‚¹ã€‚",
                "input_audio_transcription": [
                    "model": "gummy-realtime-v1"
                ],
                "turn_detection": [
                    "type": "server_vad",
                    "threshold": 0.3,
                    "silence_duration_ms": 600,
                    "prefix_padding_ms": 300
                ]
            ]
        ]

        sendEvent(sessionConfig)
    }

    // MARK: - Audio Recording

    func startRecording() {
        guard !isRecording else {
            return
        }

        do {
            print("ğŸ¤ [Omni] å¼€å§‹å½•éŸ³")

            // Stop engine if already running and remove any existing taps
            if let engine = audioEngine, engine.isRunning {
                engine.stop()
                engine.inputNode.removeTap(onBus: 0)
            }

            let audioSession = AVAudioSession.sharedInstance()

            // Allow Bluetooth to use the glasses' microphone
            try audioSession.setCategory(.playAndRecord, mode: .voiceChat, options: [.allowBluetooth, .allowBluetoothA2DP])
            try audioSession.setActive(true)

            guard let engine = audioEngine else {
                print("âŒ [Omni] éŸ³é¢‘å¼•æ“æœªåˆå§‹åŒ–")
                return
            }

            let inputNode = engine.inputNode
            let inputFormat = inputNode.outputFormat(forBus: 0)

            // Convert to PCM16 24kHz mono
            inputNode.installTap(onBus: 0, bufferSize: 4096, format: inputFormat) { [weak self] buffer, time in
                self?.processAudioBuffer(buffer)
            }

            engine.prepare()
            try engine.start()

            isRecording = true
            print("âœ… [Omni] å½•éŸ³å·²å¯åŠ¨")

        } catch {
            print("âŒ [Omni] å¯åŠ¨å½•éŸ³å¤±è´¥: \(error.localizedDescription)")
            onError?("Failed to start recording: \(error.localizedDescription)")
        }
    }

    func stopRecording() {
        guard isRecording else {
            return
        }

        print("ğŸ›‘ [Omni] åœæ­¢å½•éŸ³")
        audioEngine?.inputNode.removeTap(onBus: 0)
        audioEngine?.stop()
        isRecording = false
        hasAudioBeenSent = false
    }

    private func processAudioBuffer(_ buffer: AVAudioPCMBuffer) {
        let sourceSampleRate = buffer.format.sampleRate
        let targetSampleRate: Double = 24000
        
        // è·å–éŸ³é¢‘æ•°æ®
        guard let floatChannelData = buffer.floatChannelData else {
            return
        }
        
        let frameLength = Int(buffer.frameLength)
        let channel = floatChannelData.pointee
        
        // å¦‚æœé‡‡æ ·ç‡ä¸åŒï¼Œä½¿ç”¨ AVAudioConverter è¿›è¡Œé«˜è´¨é‡é‡é‡‡æ ·
        if sourceSampleRate != targetSampleRate, let targetFormat = targetFormat {
            // åˆ›å»ºæˆ–é‡ç”¨è½¬æ¢å™¨
            if audioConverter == nil || audioConverter?.inputFormat != buffer.format {
                audioConverter = AVAudioConverter(from: buffer.format, to: targetFormat)
            }
            
            guard let converter = audioConverter else {
                print("âŒ [Omni] æ— æ³•åˆ›å»ºéŸ³é¢‘è½¬æ¢å™¨")
                return
            }
            
            // è®¡ç®—ç›®æ ‡å¸§æ•°
            let ratio = targetSampleRate / sourceSampleRate
            let targetFrameLength = AVAudioFrameCount(ceil(Double(frameLength) * ratio))
            
            // åˆ›å»ºè¾“å‡º buffer
            guard let outputBuffer = AVAudioPCMBuffer(pcmFormat: targetFormat, frameCapacity: targetFrameLength) else {
                return
            }
            
            // è½¬æ¢
            var error: NSError?
            var inputBufferOffset: AVAudioFrameCount = 0
            
            let inputBlock: AVAudioConverterInputBlock = { inNumPackets, outStatus in
                if inputBufferOffset >= buffer.frameLength {
                    outStatus.pointee = .noDataNow
                    return nil
                }
                outStatus.pointee = .haveData
                inputBufferOffset = buffer.frameLength
                return buffer
            }
            
            converter.convert(to: outputBuffer, error: &error, withInputFrom: inputBlock)
            
            if let error = error {
                print("âŒ [Omni] éŸ³é¢‘è½¬æ¢å¤±è´¥: \(error)")
                return
            }
            
            // å°†è½¬æ¢åçš„ Float32 æ•°æ®è½¬ä¸º Int16
            guard let convertedData = outputBuffer.floatChannelData else {
                return
            }
            
            let convertedLength = Int(outputBuffer.frameLength)
            let convertedChannel = convertedData.pointee
            
            var int16Data = [Int16](repeating: 0, count: convertedLength)
            for i in 0..<convertedLength {
                let sample = convertedChannel[i]
                let clampedSample = max(-1.0, min(1.0, sample))
                int16Data[i] = Int16(clampedSample * 32767.0)
            }
            
            let data = Data(bytes: int16Data, count: convertedLength * MemoryLayout<Int16>.size)
            let base64Audio = data.base64EncodedString()
            sendAudioAppend(base64Audio)
            
        } else {
            // é‡‡æ ·ç‡å·²ç»æ˜¯ 24kHzï¼Œç›´æ¥è½¬æ¢
            var int16Data = [Int16](repeating: 0, count: frameLength)
            for i in 0..<frameLength {
                let sample = channel[i]
                let clampedSample = max(-1.0, min(1.0, sample))
                int16Data[i] = Int16(clampedSample * 32767.0)
            }
            
            let data = Data(bytes: int16Data, count: frameLength * MemoryLayout<Int16>.size)
            let base64Audio = data.base64EncodedString()
            sendAudioAppend(base64Audio)
        }

        // é€šçŸ¥ç¬¬ä¸€æ¬¡éŸ³é¢‘å·²å‘é€
        if !hasAudioBeenSent {
            hasAudioBeenSent = true
            print("âœ… [Omni] ç¬¬ä¸€æ¬¡éŸ³é¢‘å·²å‘é€ï¼ˆ\(sourceSampleRate)Hz -> \(targetSampleRate)Hzï¼‰ï¼Œå¯ç”¨è¯­éŸ³è§¦å‘æ¨¡å¼")
            DispatchQueue.main.async { [weak self] in
                self?.onFirstAudioSent?()
            }
        }
    }

    // MARK: - Send Events

    private func sendEvent(_ event: [String: Any]) {
        guard let jsonData = try? JSONSerialization.data(withJSONObject: event),
              let jsonString = String(data: jsonData, encoding: .utf8) else {
            print("âŒ [Omni] æ— æ³•åºåˆ—åŒ–äº‹ä»¶")
            return
        }

        let message = URLSessionWebSocketTask.Message.string(jsonString)
        webSocket?.send(message) { error in
            if let error = error {
                print("âŒ [Omni] å‘é€äº‹ä»¶å¤±è´¥: \(error.localizedDescription)")
                self.onError?("Send error: \(error.localizedDescription)")
            }
        }
    }

    func sendAudioAppend(_ base64Audio: String) {
        let event: [String: Any] = [
            "event_id": generateEventId(),
            "type": OmniClientEvent.inputAudioBufferAppend.rawValue,
            "audio": base64Audio
        ]
        sendEvent(event)
    }

    func sendImageAppend(_ image: UIImage) {
        guard let imageData = image.jpegData(compressionQuality: 0.6) else {
            print("âŒ [Omni] æ— æ³•å‹ç¼©å›¾ç‰‡")
            return
        }
        let base64Image = imageData.base64EncodedString()

        print("ğŸ“¸ [Omni] å‘é€å›¾ç‰‡: \(imageData.count) bytes")

        let event: [String: Any] = [
            "event_id": generateEventId(),
            "type": OmniClientEvent.inputImageBufferAppend.rawValue,
            "image": base64Image
        ]
        sendEvent(event)
    }

    func commitAudioBuffer() {
        let event: [String: Any] = [
            "event_id": generateEventId(),
            "type": OmniClientEvent.inputAudioBufferCommit.rawValue
        ]
        sendEvent(event)
    }

    // MARK: - Receive Messages

    private func receiveMessage() {
        webSocket?.receive { [weak self] result in
            guard let self = self else { return }
            
            switch result {
            case .success(let message):
                self.handleMessage(message)
                self.receiveMessage() // Continue receiving

            case .failure(let error):
                // å¦‚æœæ­£åœ¨æ–­å¼€è¿æ¥ï¼Œä¸æŠ¥å‘Šé”™è¯¯
                guard !self.isDisconnecting else {
                    print("ğŸ”Œ [Omni] æ­£å¸¸æ–­å¼€ï¼Œå¿½ç•¥æ¥æ”¶é”™è¯¯")
                    return
                }
                print("âŒ [Omni] æ¥æ”¶æ¶ˆæ¯å¤±è´¥: \(error.localizedDescription)")
                self.onError?("è¿æ¥å·²æ–­å¼€: \(error.localizedDescription)")
            }
        }
    }

    private func handleMessage(_ message: URLSessionWebSocketTask.Message) {
        switch message {
        case .string(let text):
            handleServerEvent(text)
        case .data(let data):
            if let text = String(data: data, encoding: .utf8) {
                handleServerEvent(text)
            }
        @unknown default:
            break
        }
    }

    private func handleServerEvent(_ jsonString: String) {
        guard let data = jsonString.data(using: .utf8),
              let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any],
              let type = json["type"] as? String else {
            return
        }

        // è®°å½•æ‰€æœ‰æ”¶åˆ°çš„äº‹ä»¶ç±»å‹ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        if type != "response.audio.delta" && type != "response.audio_transcript.delta" {
            print("ğŸ“© [Omni] æ”¶åˆ°äº‹ä»¶: \(type)")
        }

        DispatchQueue.main.async {
            switch type {
            case OmniServerEvent.sessionCreated.rawValue,
                 OmniServerEvent.sessionUpdated.rawValue:
                print("âœ… [Omni] ä¼šè¯å·²å»ºç«‹")
                self.onConnected?()

            case OmniServerEvent.inputAudioBufferSpeechStarted.rawValue:
                print("ğŸ¤ [Omni] æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹")
                self.onSpeechStarted?()

            case OmniServerEvent.inputAudioBufferSpeechStopped.rawValue:
                print("ğŸ›‘ [Omni] æ£€æµ‹åˆ°è¯­éŸ³åœæ­¢")
                self.onSpeechStopped?()

            case OmniServerEvent.responseAudioTranscriptDelta.rawValue:
                if let delta = json["delta"] as? String {
                    print("ğŸ’¬ [Omni] AIå›å¤ç‰‡æ®µ: \(delta)")
                    self.onTranscriptDelta?(delta)
                }

            case OmniServerEvent.responseAudioTranscriptDone.rawValue:
                let text = json["text"] as? String ?? ""
                if text.isEmpty {
                    print("âš ï¸ [Omni] AIå›å¤å®Œæˆä½†doneäº‹ä»¶æ— textå­—æ®µï¼ˆä½¿ç”¨ç´¯ç§¯çš„deltaï¼‰")
                } else {
                    print("âœ… [Omni] AIå®Œæ•´å›å¤: \(text)")
                }
                // æ€»æ˜¯è°ƒç”¨å›è°ƒï¼Œå³ä½¿textä¸ºç©ºï¼Œè®©ViewModelä½¿ç”¨ç´¯ç§¯çš„ç‰‡æ®µ
                self.onTranscriptDone?(text)

            case OmniServerEvent.responseAudioDelta.rawValue:
                if let base64Audio = json["delta"] as? String,
                   let audioData = Data(base64Encoded: base64Audio) {
                    self.onAudioDelta?(audioData)

                    // Buffer audio chunks
                    if !self.isCollectingAudio {
                        self.isCollectingAudio = true
                        self.audioBuffer = Data()
                        self.audioChunkCount = 0
                        self.hasStartedPlaying = false

                        // æ¸…é™¤ playerNode é˜Ÿåˆ—ä¸­å¯èƒ½æ®‹ç•™çš„æ—§ buffer
                        if self.isPlaybackEngineRunning {
                            // é‡è¦ï¼šreset ä¼šæ–­å¼€ playerNodeï¼Œéœ€è¦å®Œå…¨é‡æ–°åˆå§‹åŒ–
                            self.stopPlaybackEngine()
                            self.setupPlaybackEngine()
                            self.startPlaybackEngine()
                            self.playerNode?.play()
                            print("ğŸ”„ [Omni] é‡æ–°åˆå§‹åŒ–æ’­æ”¾å¼•æ“")
                        }
                    }

                    self.audioChunkCount += 1

                    // æµå¼æ’­æ”¾ç­–ç•¥ï¼šæ”¶é›†å°‘é‡ç‰‡æ®µåå¼€å§‹æµå¼è°ƒåº¦
                    if !self.hasStartedPlaying {
                        // é¦–æ¬¡æ’­æ”¾å‰ï¼šå…ˆæ”¶é›†
                        self.audioBuffer.append(audioData)

                        if self.audioChunkCount >= self.minChunksBeforePlay {
                            // å·²æ”¶é›†è¶³å¤Ÿç‰‡æ®µï¼Œå¼€å§‹æ’­æ”¾
                            self.hasStartedPlaying = true
                            self.playAudio(self.audioBuffer)
                            self.audioBuffer = Data()
                        }
                    } else {
                        // å·²å¼€å§‹æ’­æ”¾ï¼šç›´æ¥è°ƒåº¦æ¯ä¸ªç‰‡æ®µï¼ŒAVAudioPlayerNode ä¼šè‡ªåŠ¨æ’é˜Ÿ
                        self.playAudio(audioData)
                    }
                }

            case OmniServerEvent.responseAudioDone.rawValue:
                self.isCollectingAudio = false

                // Play remaining buffered audio (if any)
                if !self.audioBuffer.isEmpty {
                    self.playAudio(self.audioBuffer)
                    self.audioBuffer = Data()
                }

                self.audioChunkCount = 0
                self.hasStartedPlaying = false
                self.onAudioDone?()

            case OmniServerEvent.conversationItemInputAudioTranscriptionCompleted.rawValue:
                // ç”¨æˆ·è¯­éŸ³è¯†åˆ«å®Œæˆ
                if let transcript = json["transcript"] as? String {
                    print("ğŸ‘¤ [Omni] ç”¨æˆ·è¯´: \(transcript)")
                    self.onUserTranscript?(transcript)
                }

            case OmniServerEvent.conversationItemCreated.rawValue:
                // å¯èƒ½åŒ…å«å…¶ä»–ç±»å‹çš„ä¼šè¯é¡¹
                break

            case OmniServerEvent.error.rawValue:
                if let error = json["error"] as? [String: Any],
                   let message = error["message"] as? String {
                    print("âŒ [Omni] æœåŠ¡å™¨é”™è¯¯: \(message)")
                    self.onError?(message)
                }

            default:
                break
            }
        }
    }

    // MARK: - Audio Playback (AVAudioEngine + AVAudioPlayerNode)

    private func playAudio(_ audioData: Data) {
        guard let playerNode = playerNode,
              let audioFormat = audioFormat else {
            return
        }

        // Start playback engine if not running
        if !isPlaybackEngineRunning {
            startPlaybackEngine()
            playerNode.play()
        } else {
            // ç¡®ä¿ playerNode åœ¨è¿è¡Œ
            if !playerNode.isPlaying {
                playerNode.play()
            }
        }

        // Convert PCM16 Data to AVAudioPCMBuffer
        guard let pcmBuffer = createPCMBuffer(from: audioData, format: audioFormat) else {
            return
        }

        // Schedule buffer for playback
        playerNode.scheduleBuffer(pcmBuffer)
    }

    private func createPCMBuffer(from data: Data, format: AVAudioFormat) -> AVAudioPCMBuffer? {
        // Calculate frame count (each frame is 2 bytes for PCM16 mono)
        let frameCount = data.count / 2

        guard let buffer = AVAudioPCMBuffer(pcmFormat: format, frameCapacity: AVAudioFrameCount(frameCount)),
              let channelData = buffer.int16ChannelData else {
            return nil
        }

        buffer.frameLength = AVAudioFrameCount(frameCount)

        // Copy PCM16 data directly to buffer
        data.withUnsafeBytes { (bytes: UnsafeRawBufferPointer) in
            guard let baseAddress = bytes.baseAddress else { return }
            let int16Pointer = baseAddress.assumingMemoryBound(to: Int16.self)
            channelData[0].update(from: int16Pointer, count: frameCount)
        }

        return buffer
    }

    // MARK: - Helpers

    private func generateEventId() -> String {
        eventIdCounter += 1
        return "event_\(eventIdCounter)_\(UUID().uuidString.prefix(8))"
    }
}

// MARK: - URLSessionWebSocketDelegate

extension OmniRealtimeService: URLSessionWebSocketDelegate {
    func urlSession(_ session: URLSession, webSocketTask: URLSessionWebSocketTask, didOpenWithProtocol protocol: String?) {
        print("âœ… [Omni] WebSocket è¿æ¥å·²å»ºç«‹, protocol: \(`protocol` ?? "none")")
    }

    func urlSession(_ session: URLSession, webSocketTask: URLSessionWebSocketTask, didCloseWith closeCode: URLSessionWebSocketTask.CloseCode, reason: Data?) {
        let reasonString = reason.flatMap { String(data: $0, encoding: .utf8) } ?? "unknown"
        print("ğŸ”Œ [Omni] WebSocket å·²æ–­å¼€, closeCode: \(closeCode.rawValue), reason: \(reasonString)")
        
        // å¦‚æœä¸æ˜¯ä¸»åŠ¨æ–­å¼€ï¼Œåˆ™é€šçŸ¥è°ƒç”¨æ–¹
        if !isDisconnecting {
            DispatchQueue.main.async { [weak self] in
                self?.onDisconnected?("closeCode: \(closeCode.rawValue), reason: \(reasonString)")
            }
        }
    }
}
